{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size: int = None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.inverse_vocab = {}\n",
    "        self.pattern = None\n",
    "        self.original_text = \"\"\n",
    "    \n",
    "    def train_from_file(self, file_path: str, encoding='utf-8', auto_vocab_size=True):\n",
    "        \"\"\"Train the tokenizer from a text file and calculate compression ratio.\"\"\"\n",
    "        print(f\"Loading text file: {file_path}\")\n",
    "        \n",
    "        # Store the original text for compression analysis\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            self.original_text = f.read()\n",
    "            \n",
    "        if auto_vocab_size or self.vocab_size is None:\n",
    "            self.vocab_size = self._suggest_vocab_size(self.original_text)\n",
    "            print(f\"\\nUsing automatically determined vocabulary size: {self.vocab_size}\")\n",
    "            \n",
    "        print(\"\\nTraining tokenizer...\")\n",
    "        self.train(self.original_text.split('\\n'))\n",
    "        \n",
    "        # Calculate and print compression metrics for training data\n",
    "        self.print_training_compression_stats()\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train the BPE tokenizer on a list of texts.\"\"\"\n",
    "        # Count word frequencies\n",
    "        print(\"Counting word frequencies...\")\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_freqs[' '.join(list(word)) + ' </w>'] += 1\n",
    "        \n",
    "        # Initialize characters vocabulary\n",
    "        chars = set()\n",
    "        for word in self.word_freqs.keys():\n",
    "            chars.update(word.split())\n",
    "        self.vocab = {char: idx for idx, char in enumerate(chars)}\n",
    "        self.inverse_vocab = {idx: char for char, idx in self.vocab.items()}\n",
    "        \n",
    "        # Main BPE training loop\n",
    "        print(f\"Starting BPE training with target vocabulary size: {self.vocab_size}\")\n",
    "        num_merges = min(self.vocab_size - len(self.vocab), 1000)\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats()\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Completed {i + 1} merges...\")\n",
    "                \n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            self.merge_vocab(best_pair)\n",
    "            \n",
    "            # Add the merged pair to vocabulary\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "            self.inverse_vocab[len(self.vocab) - 1] = merged_token\n",
    "            self.merges[best_pair] = merged_token\n",
    "        \n",
    "        # Create regex pattern for tokenization\n",
    "        self.pattern = re.compile(\"|\".join(map(re.escape, sorted(self.vocab.keys(), key=len, reverse=True))))\n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def get_stats(self) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Count frequency of adjacent pairs in current vocabulary.\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair: Tuple[str, str]):\n",
    "        \"\"\"Merge a pair of symbols in the vocabulary.\"\"\"\n",
    "        bigram = \" \".join(pair)\n",
    "        replacement = \"\".join(pair)\n",
    "        \n",
    "        # Create a new dictionary to store updated frequencies\n",
    "        new_word_freqs = defaultdict(int)\n",
    "        \n",
    "        # Iterate over a copy of the keys\n",
    "        for word in list(self.word_freqs.keys()):\n",
    "            if bigram in word:\n",
    "                new_word = word.replace(bigram, replacement)\n",
    "                new_word_freqs[new_word] += self.word_freqs[word]\n",
    "            else:\n",
    "                new_word_freqs[word] += self.word_freqs[word]\n",
    "                \n",
    "        self.word_freqs = new_word_freqs\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text into token ids.\"\"\"\n",
    "        tokens = []\n",
    "        for word in text.split():\n",
    "            word = \" \".join(list(word)) + \" </w>\"\n",
    "            \n",
    "            # Find all non-overlapping matches\n",
    "            current_word = word\n",
    "            encoded_word = []\n",
    "            \n",
    "            while current_word:\n",
    "                match = self.pattern.search(current_word)\n",
    "                if not match:\n",
    "                    if current_word.strip():\n",
    "                        print(f\"Warning: Could not encode '{current_word.strip()}'\")\n",
    "                    break\n",
    "                    \n",
    "                token = match.group(0)\n",
    "                encoded_word.append(self.vocab[token])\n",
    "                start, end = match.span()\n",
    "                current_word = current_word[end:].strip()\n",
    "            \n",
    "            tokens.extend(encoded_word)\n",
    "        \n",
    "        self.total_tokens = len(tokens)  # Update token count\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Decode token ids back to text.\"\"\"\n",
    "        text = []\n",
    "        current_word = []\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.inverse_vocab:\n",
    "                print(f\"Warning: Unknown token ID {token_id}\")\n",
    "                continue\n",
    "                \n",
    "            token = self.inverse_vocab[token_id]\n",
    "            if token == \"</w>\":\n",
    "                text.append(\"\".join(current_word))\n",
    "                current_word = []\n",
    "            else:\n",
    "                current_word.append(token)\n",
    "                \n",
    "        if current_word:  # Handle case where last token wasn't </w>\n",
    "            text.append(\"\".join(current_word))\n",
    "            \n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"Print tokenizer statistics including compression ratio.\"\"\"\n",
    "        print(\"\\nTokenizer Statistics:\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Number of merges: {len(self.merges)}\")\n",
    "        if self.total_tokens > 0 and self.original_chars > 0:\n",
    "            compression_ratio = self.original_chars / (self.total_tokens * 2)  # Assuming 2 bytes per token on average\n",
    "            print(f\"Original characters: {self.original_chars:,}\")\n",
    "            print(f\"Total tokens: {self.total_tokens:,}\")\n",
    "            print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save tokenizer configuration to file.\"\"\"\n",
    "        config = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'BPETokenizer':\n",
    "        \"\"\"Load tokenizer configuration from file.\"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "            \n",
    "        tokenizer = cls(vocab_size=config['vocab_size'])\n",
    "        tokenizer.vocab = config['vocab']\n",
    "        tokenizer.merges = config['merges']\n",
    "        tokenizer.inverse_vocab = {idx: char for char, idx in tokenizer.vocab.items()}\n",
    "        tokenizer.pattern = re.compile(\"|\".join(map(re.escape, sorted(tokenizer.vocab.keys(), key=len, reverse=True))))\n",
    "        return tokenizer\n",
    "    \n",
    "    \n",
    "    def analyze_text(self, file_path: str, encoding='utf-8') -> int:\n",
    "        \"\"\"Analyze text file and suggest vocabulary size.\"\"\"\n",
    "        print(f\"Analyzing text file: {file_path}\")\n",
    "        \n",
    "        # Read file\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        # Basic text statistics\n",
    "        total_chars = len(text)\n",
    "        unique_chars = len(set(text))\n",
    "        total_words = len(text.split())\n",
    "        unique_words = len(set(text.split()))\n",
    "        \n",
    "        # Calculate suggested vocab size\n",
    "        # Rule of thumb: sqrt(unique_words) * log(total_words)\n",
    "        suggested_size = int(math.sqrt(unique_words) * math.log(total_words + 1))\n",
    "        # Round to nearest hundred and ensure minimum size\n",
    "        suggested_size = max(100, round(suggested_size, -2))\n",
    "        \n",
    "        print(\"\\nText Analysis:\")\n",
    "        print(f\"Total characters: {total_chars:,}\")\n",
    "        print(f\"Unique characters: {unique_chars}\")\n",
    "        print(f\"Total words: {total_words:,}\")\n",
    "        print(f\"Unique words: {unique_words:,}\")\n",
    "        print(f\"Suggested vocabulary size: {suggested_size:,}\")\n",
    "        \n",
    "        return suggested_size\n",
    "    \n",
    "    def calculate_compression_metrics(self, text: str) -> dict:\n",
    "        \"\"\"Calculate detailed compression metrics for a given text.\"\"\"\n",
    "        # Original text metrics\n",
    "        original_bytes = len(text.encode('utf-8'))\n",
    "        original_chars = len(text)\n",
    "        \n",
    "        # Encode the text\n",
    "        token_ids = self.encode(text)\n",
    "        \n",
    "        # Calculate encoded size\n",
    "        # Each token ID needs enough bits to represent the vocab size\n",
    "        bits_per_token = max(math.ceil(math.log2(len(self.vocab))), 8)\n",
    "        encoded_bits = len(token_ids) * bits_per_token\n",
    "        encoded_bytes = math.ceil(encoded_bits / 8)\n",
    "        \n",
    "        metrics = {\n",
    "            'original_size': {\n",
    "                'bytes': original_bytes,\n",
    "                'chars': original_chars\n",
    "            },\n",
    "            'encoded_size': {\n",
    "                'tokens': len(token_ids),\n",
    "                'bits_per_token': bits_per_token,\n",
    "                'total_bits': encoded_bits,\n",
    "                'bytes': encoded_bytes\n",
    "            },\n",
    "            'compression': {\n",
    "                'ratio': original_bytes / encoded_bytes,\n",
    "                'space_saving_percent': (1 - (encoded_bytes / original_bytes)) * 100\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def print_training_compression_stats(self):\n",
    "        \"\"\"Calculate and print compression statistics for training data.\"\"\"\n",
    "        if not self.original_text:\n",
    "            print(\"No training data available for compression analysis\")\n",
    "            return\n",
    "            \n",
    "        # Original text metrics\n",
    "        original_bytes = len(self.original_text.encode('utf-8'))\n",
    "        original_chars = len(self.original_text)\n",
    "        \n",
    "        # Encode the entire training text\n",
    "        encoded_tokens = self.encode(self.original_text)\n",
    "        \n",
    "        # Calculate bits needed per token based on vocab size\n",
    "        bits_per_token = max(math.ceil(math.log2(len(self.vocab))), 8)\n",
    "        encoded_bits = len(encoded_tokens) * bits_per_token\n",
    "        encoded_bytes = math.ceil(encoded_bits / 8)\n",
    "        \n",
    "        # Calculate compression ratio\n",
    "        compression_ratio = original_bytes / encoded_bytes\n",
    "        space_saving = (1 - (encoded_bytes / original_bytes)) * 100\n",
    "        \n",
    "        print(\"\\nTraining Data Compression Analysis:\")\n",
    "        print(f\"Original text size:\")\n",
    "        print(f\"  - Characters: {original_chars:,}\")\n",
    "        print(f\"  - Bytes: {original_bytes:,}\")\n",
    "        \n",
    "        print(f\"\\nTokenized text size:\")\n",
    "        print(f\"  - Vocabulary size: {len(self.vocab):,}\")\n",
    "        print(f\"  - Total tokens: {len(encoded_tokens):,}\")\n",
    "        print(f\"  - Bits per token: {bits_per_token}\")\n",
    "        print(f\"  - Total bits: {encoded_bits:,}\")\n",
    "        print(f\"  - Bytes: {encoded_bytes:,}\")\n",
    "        \n",
    "        print(f\"\\nCompression metrics:\")\n",
    "        print(f\"  - Compression ratio: {compression_ratio:.2f}x\")\n",
    "        print(f\"  - Space saving: {space_saving:.1f}%\")\n",
    "        print(f\"  - Average tokens per character: {len(encoded_tokens)/original_chars:.2f}\")\n",
    "    \n",
    "    def _suggest_vocab_size(self, text: str) -> int:\n",
    "        \"\"\"Suggest vocabulary size based on text statistics.\"\"\"\n",
    "        words = text.split()\n",
    "        unique_words = len(set(words))\n",
    "        total_words = len(words)\n",
    "        \n",
    "        suggested_size = int(math.sqrt(unique_words) * math.log(total_words + 1))\n",
    "        return max(100, round(suggested_size, -2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "class HindiBPE:\n",
    "    def __init__(self, vocab_size: int = 5000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = set()\n",
    "        self.merge_rules = {}\n",
    "        self.token_counts = Counter()\n",
    "        \n",
    "    def get_stats(self, pairs: List[Tuple[str, str]]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Count frequency of pairs in the current vocabulary\"\"\"\n",
    "        stats = defaultdict(int)\n",
    "        for pair in pairs:\n",
    "            stats[pair] += 1\n",
    "        return stats\n",
    "    \n",
    "    def get_pairs(self, word: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Get all adjacent pairs in a word\"\"\"\n",
    "        return [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
    "    \n",
    "    def merge_vocab(self, pair: Tuple[str, str], v_in: List[str]) -> List[str]:\n",
    "        \"\"\"Merge a pair of tokens in the vocabulary\"\"\"\n",
    "        v_out = []\n",
    "        i = 0\n",
    "        while i < len(v_in):\n",
    "            if i < len(v_in) - 1 and v_in[i] == pair[0] and v_in[i+1] == pair[1]:\n",
    "                v_out.append(pair[0] + pair[1])\n",
    "                i += 2\n",
    "            else:\n",
    "                v_out.append(v_in[i])\n",
    "                i += 1\n",
    "        return v_out\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \n",
    "        # Count word frequencies in parallel\n",
    "        word_freqs = Counter()\n",
    "        for text in tqdm(texts, desc=\"Counting words\"):\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word = '▁' + word  # Add space marker\n",
    "                word_freqs[word] += 1\n",
    "        \n",
    "        print(f\"Found {len(word_freqs)} unique words\")        \n",
    "        # Initialize vocabulary with characters\n",
    "        for word, freq in word_freqs.items():\n",
    "            # if freq < min_freq:\n",
    "            #     continue\n",
    "            for char in word:\n",
    "                self.vocab.add(char)\n",
    "        \n",
    "        # Convert words to list of characters\n",
    "        splits = {word: list(word) for word in word_freqs.keys()}\n",
    "        \n",
    "        # Main training loop with progress bar\n",
    "        pbar = tqdm(total=min(self.vocab_size - len(self.vocab), len(word_freqs)), desc=\"Training BPE\")\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = defaultdict(int)\n",
    "            for word, freq in word_freqs.items():\n",
    "                # if freq < min_freq:\n",
    "                #     continue\n",
    "                word_pairs = self.get_pairs(splits[word])\n",
    "                for pair in word_pairs:\n",
    "                    pairs[pair] += freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            self.vocab.add(''.join(best_pair))\n",
    "            self.merge_rules[best_pair] = ''.join(best_pair)\n",
    "            \n",
    "            new_splits = {}\n",
    "            for word in splits:\n",
    "                new_splits[word] = self.merge_vocab(best_pair, splits[word])\n",
    "            splits = new_splits\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Update token counts after training\n",
    "        self._update_token_counts(texts)\n",
    "            \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text using learned BPE rules\"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word = '▁' + word  # Add space marker\n",
    "            current_tokens = list(word)\n",
    "            \n",
    "            while True:\n",
    "                pairs = self.get_pairs(current_tokens)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                    \n",
    "                # Find mergeable pair\n",
    "                mergeable = False\n",
    "                for pair in pairs:\n",
    "                    if pair in self.merge_rules:\n",
    "                        current_tokens = self.merge_vocab(pair, current_tokens)\n",
    "                        mergeable = True\n",
    "                        break\n",
    "                \n",
    "                if not mergeable:\n",
    "                    break\n",
    "                    \n",
    "            tokens.extend(current_tokens)\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save the tokenizer to a file\"\"\"\n",
    "        save_dict = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'vocab': list(self.vocab),\n",
    "            'merge_rules': {str(k): v for k, v in self.merge_rules.items()},\n",
    "            'token_counts': dict(self.token_counts)\n",
    "        }\n",
    "        \n",
    "        path = Path(path)\n",
    "        # Save main model\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "            \n",
    "        # Save token statistics separately as JSON for easy viewing\n",
    "        stats_path = path.with_suffix('.stats.json')\n",
    "        stats = {\n",
    "            'vocab_size': len(self.vocab),\n",
    "            'token_counts': dict(self.token_counts.most_common(100)),  # Save top 100 tokens\n",
    "            'total_tokens': sum(self.token_counts.values())\n",
    "        }\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load a saved tokenizer\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            save_dict = pickle.load(f)\n",
    "            \n",
    "        tokenizer = cls(vocab_size=save_dict['vocab_size'])\n",
    "        tokenizer.vocab = set(save_dict['vocab'])\n",
    "        tokenizer.merge_rules = {tuple(eval(k)): v for k, v in save_dict['merge_rules'].items()}\n",
    "        tokenizer.token_counts = Counter(save_dict['token_counts'])\n",
    "        return tokenizer\n",
    "\n",
    "    def _update_token_counts(self, texts: List[str]):\n",
    "        \"\"\"Update token count statistics\"\"\"\n",
    "        self.token_counts.clear()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            self.token_counts.update(tokens)\n",
    "\n",
    "    def get_token_stats(self) -> Dict:\n",
    "        \"\"\"Get token statistics\"\"\"\n",
    "        total_tokens = sum(self.token_counts.values())\n",
    "        unique_tokens = len(self.token_counts)\n",
    "        most_common = self.token_counts.most_common(20)\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'unique_tokens': unique_tokens,\n",
    "            'vocab_size': len(self.vocab),\n",
    "            'most_common_tokens': [\n",
    "                {'token': token, 'count': count, 'percentage': count/total_tokens*100}\n",
    "                for token, count in most_common\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "def load_text_file(file_path: str, encoding: str = 'utf-8') -> List[str]:\n",
    "    \"\"\"Load text from a file and return a list of lines.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            lines = [line.strip() for line in file.readlines()]\n",
    "            lines = [line for line in lines if line]\n",
    "            return lines\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error: Could not decode file with {encoding} encoding.\")\n",
    "        print(\"Try using a different encoding (e.g., 'utf-8-sig' for files with BOM)\")\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def calculate_compression_ratio(original_texts: List[str], tokenized_texts: List[List[str]]) -> float:\n",
    "    \"\"\"Calculate compression ratio (original characters / tokens)\"\"\"\n",
    "    total_chars = sum(len(text) for text in original_texts)\n",
    "    total_tokens = sum(len(tokens) for tokens in tokenized_texts)\n",
    "    return total_chars / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load text from file\n",
    "# file_path = r\"C:\\Users\\vasal\\Study\\TSAI\\TSAI-11\\hin_news_2010_30K\\hin_news_2010_30K-sentences.txt\"  # Replace with your file path\n",
    "# texts = load_text_file(file_path)\n",
    "\n",
    "# # Initialize and train BPE\n",
    "# bpe = HindiBPE(vocab_size=4500)\n",
    "# bpe.train(texts)\n",
    "\n",
    "# # Test tokenization and compression ratio\n",
    "# tokenized_texts = [bpe.tokenize(text) for text in texts]\n",
    "# compression_ratio = calculate_compression_ratio(texts, tokenized_texts)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Loaded {len(texts)} lines of text\")\n",
    "# print(f\"Vocabulary size: {len(bpe.vocab)}\")\n",
    "# print(f\"Compression ratio: {compression_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|██████████| 30000/30000 [00:00<00:00, 172174.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74835 unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE: 100%|██████████| 4385/4385 [15:28<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Statistics:\n",
      "Total tokens processed: 1,278,670\n",
      "Unique tokens: 2,743\n",
      "Vocabulary size: 4,500\n",
      "\n",
      "Most common tokens:\n",
      "ं: 32,111 (2.51%)\n",
      "▁।: 29,497 (2.31%)\n",
      "▁के: 26,057 (2.04%)\n",
      "र: 23,741 (1.86%)\n",
      "ा: 22,920 (1.79%)\n",
      "▁मे: 20,373 (1.59%)\n",
      "े: 20,052 (1.57%)\n",
      "▁की: 16,136 (1.26%)\n",
      "ी: 15,424 (1.21%)\n",
      "▁है: 14,084 (1.10%)\n",
      "\n",
      "Tokenizer saved to hindi_bpe_model.pkl\n",
      "Token statistics saved to hindi_bpe_model.stats.json\n",
      "\n",
      "Successfully loaded the saved tokenizer\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\vasal\\Study\\TSAI\\TSAI-11\\hin_news_2010_30K\\hin_news_2010_30K-sentences.txt\"\n",
    "texts = load_text_file(file_path)\n",
    "\n",
    "# Train tokenizer\n",
    "bpe = HindiBPE(vocab_size=4500)\n",
    "bpe.train(texts)  # Use 4 workers for parallel processing\n",
    "\n",
    "# Get and print token statistics\n",
    "stats = bpe.get_token_stats()\n",
    "print(\"\\nToken Statistics:\")\n",
    "print(f\"Total tokens processed: {stats['total_tokens']:,}\")\n",
    "print(f\"Unique tokens: {stats['unique_tokens']:,}\")\n",
    "print(f\"Vocabulary size: {stats['vocab_size']:,}\")\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token_info in stats['most_common_tokens'][:10]:  # Show top 10\n",
    "    print(f\"{token_info['token']}: {token_info['count']:,} ({token_info['percentage']:.2f}%)\")\n",
    "\n",
    "# Save the tokenizer\n",
    "bpe.save(\"hindi_bpe_model.pkl\")\n",
    "print(\"\\nTokenizer saved to hindi_bpe_model.pkl\")\n",
    "print(\"Token statistics saved to hindi_bpe_model.stats.json\")\n",
    "\n",
    "# Example of loading the saved tokenizer\n",
    "loaded_bpe = HindiBPE.load(\"hindi_bpe_model.pkl\")\n",
    "print(\"\\nSuccessfully loaded the saved tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [bpe.tokenize(text) for text in texts]\n",
    "compression_ratio = calculate_compression_ratio(texts, tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.514001266941431"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
